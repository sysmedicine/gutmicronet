{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076af349-8983-4798-9d21-656d9fa8090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "import numpy as np\n",
    "import os,re, leidenalg, pickle\n",
    "import igraph as ig\n",
    "import statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eced3be-bdab-42c1-a36d-bc4d829c74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_Analysis:\n",
    "    def __init__(self,raw_data,nodes,name,respath):\n",
    "        self.res_path=respath\n",
    "        self.name = name\n",
    "\n",
    "        self.network_ori=self.__calc(raw_data)\n",
    "        self.nodes=nodes\n",
    "        print('Network Analysis')\n",
    "        self.__net_analysis_combi()\n",
    "    \n",
    "    def __calc(self,df):\n",
    "        print('Calculating Correlation..')\n",
    "        temp=spearmanr(df.T, nan_policy='omit')\n",
    "        corr=pd.DataFrame(temp[0],columns=list(df.index),index=list(df.index))\n",
    "        pval=pd.DataFrame(temp[1],columns=list(df.index),index=list(df.index))\n",
    "        print('Filtering the matrix Correlation..')\n",
    "        corr=corr.where(np.triu(np.ones(corr.shape)).astype(np.bool))\n",
    "        pval=pval.where(np.triu(np.ones(pval.shape)).astype(np.bool))\n",
    "        print('Making long table of Correlation..')\n",
    "        corr2=corr.unstack().reset_index(name='weight')\n",
    "        pval2=pval.unstack().reset_index(name='pval')\n",
    "        res=corr2.merge(pval2,on=['level_0','level_1'])\n",
    "        res=res[res['level_0'] != res['level_1']]\n",
    "        res=res.dropna()\n",
    "        res=res[['level_0','level_1','weight','pval']]\n",
    "        res['padj']=multipletests(res['pval'],method='fdr_bh')[1]\n",
    "        res.columns=['source','target','correlation','pvalue','padj']\n",
    "        res=res[res.padj < 0.05].reset_index(drop=True)\n",
    "        res.to_csv('%s/%s_edges.txt' % (self.res_path, self.name),sep='\\t')\n",
    "        print('Done!!')\n",
    "        return res\n",
    "    \n",
    "    def __net_analysis_combi(self):\n",
    "        print('Loading The Network...')\n",
    "        temp=self.network_ori\n",
    "        g= ig.Graph.TupleList(zip(temp['source'],temp['target'],temp['correlation']),weights=True)\n",
    "        self.network = g\n",
    "        G_pos = g.subgraph_edges(g.es.select(weight_gt = 0), delete_vertices=False)\n",
    "        G_neg = g.subgraph_edges(g.es.select(weight_lt = 0), delete_vertices=False)\n",
    "        G_neg.es['weight'] = [-w for w in G_neg.es['weight']]\n",
    "        part_pos = leidenalg.ModularityVertexPartition(G_pos, weights='weight')\n",
    "        part_neg = leidenalg.ModularityVertexPartition(G_neg, weights='weight');\n",
    "        optimiser = leidenalg.Optimiser()\n",
    "        diff = optimiser.optimise_partition_multiplex([part_pos, part_neg],layer_weights=[1,-1], n_iterations=-1)\n",
    "        self.clustering_combi=pd.DataFrame(pd.Series(part_pos.membership+part_neg.membership,index=G_pos.vs['name']+G_neg.vs['name'])).reset_index().drop_duplicates().set_index('index')[0]\n",
    "        print('Cluster Analysis...')\n",
    "        self.modularity_combi=diff\n",
    "        self.nodes['cluster'] = self.clustering_combi.reindex(self.nodes.index).tolist()\n",
    "        temp = pd.Series(self.network.degree(),index = self.network.vs['name'],name='Degree').reindex(self.nodes.index)\n",
    "        self.nodes = pd.concat([self.nodes,temp],1)\n",
    "        self.nodes.to_csv('%s/%s_nodes.txt' % (self.res_path,self.name),sep='\\t')\n",
    "    \n",
    "    def save_network(self):\n",
    "        print('Saving The Network..')\n",
    "        pickle_out = open('%s/%s_network_object.pkl' % (self.res_path, self.name),\"wb\")\n",
    "        self.writer = None\n",
    "        pickle.dump(self, pickle_out)\n",
    "        pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea61b5d-fcb0-4a35-95f2-6c39f0e61b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = 5 # Minimum samples\n",
    "\n",
    "conds = pd.read_csv('https://www.microbiomeatlas.org/data/HGMA.web.metadata.csv',index_col = 'sample.ID')\n",
    "conds['combi_all'] = conds['Gender'].astype(str)+conds['Geography']#+conds['type']\n",
    "cohorts = pd.read_excel('SupplementaryTable1.xlsx', index_col = 'Dataset ID')\n",
    "conds = conds.merge(cohorts, left_on = 'dataset.ID', right_index = True)\n",
    "data = pd.read_csv('https://www.microbiomeatlas.org/data/HGMA.web.MSP.abundance.matrix.csv', index_col = 0)[conds.index]\n",
    "data = data.replace(0,np.nan).dropna(thresh = thr)#.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37862e-72f5-45d4-ba4e-d9b5c6290a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = 'ALL'\n",
    "k = Network_Analysis(data.fillna(0), nodes, name = name ,respath='Networks_new/')\n",
    "k.save_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7466f1-7084-4a60-b173-b24ff737cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in conds['type'].unique():\n",
    "    \n",
    "    conds_sub = conds[(conds['type'] == var)]\n",
    "    data_sub = data[conds_sub.index]\n",
    "    data_sub = data_sub.dropna(thresh = thr)\n",
    "    k = Network_Analysis(data_sub.fillna(0), nodes, name = var ,respath='Networks_new/')\n",
    "    k.save_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e094b2ea-5d13-4b68-8c98-16aed2fd70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in conds['Region'].unique():\n",
    "    conds_sub = conds[(conds['Region'] == var)]\n",
    "    data_sub = data[conds_sub.index]\n",
    "    data_sub = data_sub.dropna(thresh = thr)\n",
    "    k = Network_Analysis(data_sub.fillna(0), nodes, name = var ,respath='Networks_new/')\n",
    "    k.save_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7810511-d47c-4149-a96d-2a39d3b6d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in conds['Gender'].unique():\n",
    "    if type(var) == float:\n",
    "        continue\n",
    "    conds_sub = conds[(conds['Gender'] == var)]\n",
    "    data_sub = data[conds_sub.index]\n",
    "    data_sub = data_sub.dropna(thresh = thr)\n",
    "    k = Network_Analysis(data_sub.fillna(0), nodes, name = var ,respath='Networks_new/')\n",
    "    k.save_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c826d6-20d0-43c5-80e7-4c3b66866e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in conds['Cohort_Arif'].unique():\n",
    "    if type(var) == float:\n",
    "        continue\n",
    "    conds_sub = conds[(conds['Cohort_Arif'] == var)]\n",
    "    data_sub = data[conds_sub.index]\n",
    "    data_sub = data_sub.dropna(thresh = thr)\n",
    "    k = Network_Analysis(data_sub.fillna(0), nodes, name = var ,respath='Networks_new/')\n",
    "    k.save_network()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
